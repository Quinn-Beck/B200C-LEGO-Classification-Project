{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import squeezenet1_1, resnet50, densenet201\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from preprocessing_pipeline import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12000/12000 [01:49<00:00, 109.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = get_data(num_classes=num_classes)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1500 [06:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Batch: 0 / Epoch: 0 / Loss: 1.9132 / Pred:tensor([3.1362, 3.9493, 5.1024], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100 / Epoch: 0 / Loss: 0.9641 / Pred:tensor([1.1034, 3.3354, 1.9352], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 0 / Loss: 0.7433 / Pred:tensor([2.6188, 0.2852, 2.0602], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 2/20\n",
      "Batch: 0 / Epoch: 1 / Loss: 0.5441 / Pred:tensor([7.8896, 0.0409, 3.3976], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 1 / Loss: 0.5054 / Pred:tensor([5.7580, 0.7011, 0.9782], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 1 / Loss: 0.5552 / Pred:tensor([0.2149, 3.6262, 0.5827], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 3/20\n",
      "Batch: 0 / Epoch: 2 / Loss: 0.4669 / Pred:tensor([0.2593, 6.8310, 1.0293], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 2 / Loss: 0.3665 / Pred:tensor([7.0009, 0.1938, 1.4598], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 2 / Loss: 0.3184 / Pred:tensor([7.3259, 0.0000, 1.8489], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 4/20\n",
      "Batch: 0 / Epoch: 3 / Loss: 0.2881 / Pred:tensor([0.9997, 1.5624, 1.5205], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 3 / Loss: 0.2181 / Pred:tensor([0.7107, 6.4289, 2.7569], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 3 / Loss: 0.0731 / Pred:tensor([0.1512, 6.9318, 1.7558], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 5/20\n",
      "Batch: 0 / Epoch: 4 / Loss: 0.2924 / Pred:tensor([0.4083, 7.6509, 3.2563], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 4 / Loss: 0.2701 / Pred:tensor([2.8829, 6.9831, 0.0331], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 4 / Loss: 0.2297 / Pred:tensor([1.7054, 4.4257, 4.4539], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 6/20\n",
      "Batch: 0 / Epoch: 5 / Loss: 0.2790 / Pred:tensor([0.9943, 1.6110, 5.5927], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 5 / Loss: 0.1614 / Pred:tensor([7.0041, 0.1681, 4.1434], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 5 / Loss: 0.1136 / Pred:tensor([2.8612, 0.3797, 8.8852], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 7/20\n",
      "Batch: 0 / Epoch: 6 / Loss: 0.4851 / Pred:tensor([1.3873, 6.5777, 5.5762], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 6 / Loss: 0.0819 / Pred:tensor([9.8259, 0.0000, 3.0897], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 6 / Loss: 0.0493 / Pred:tensor([4.6524, 2.0769, 7.9088], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 8/20\n",
      "Batch: 0 / Epoch: 7 / Loss: 0.0940 / Pred:tensor([ 2.4274, 12.7776,  0.9067], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 7 / Loss: 0.1354 / Pred:tensor([4.4863, 0.8431, 8.2506], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 7 / Loss: 0.0500 / Pred:tensor([0.6681, 5.6682, 4.5779], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 9/20\n",
      "Batch: 0 / Epoch: 8 / Loss: 0.1769 / Pred:tensor([7.6397, 1.4462, 3.3888], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 8 / Loss: 0.0574 / Pred:tensor([ 3.6653, 11.8344,  1.8190], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 8 / Loss: 0.0254 / Pred:tensor([11.7176,  0.8672,  3.9212], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 10/20\n",
      "Batch: 0 / Epoch: 9 / Loss: 0.0258 / Pred:tensor([ 1.0173,  1.3046, 12.5910], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 9 / Loss: 0.0170 / Pred:tensor([ 5.6273, 13.7285,  2.2335], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 9 / Loss: 0.1805 / Pred:tensor([ 6.3452,  4.9187, 11.2855], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 11/20\n",
      "Batch: 0 / Epoch: 10 / Loss: 0.0722 / Pred:tensor([ 2.5091, 16.4521,  1.3281], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 10 / Loss: 0.0238 / Pred:tensor([ 2.6584, 12.3153,  1.3173], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 10 / Loss: 0.0553 / Pred:tensor([ 1.2714, 14.0849,  2.4795], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 12/20\n",
      "Batch: 0 / Epoch: 11 / Loss: 0.0495 / Pred:tensor([1.4840, 2.0674, 9.4361], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 11 / Loss: 0.4605 / Pred:tensor([ 2.5943,  2.0266, 11.5128], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 11 / Loss: 0.0520 / Pred:tensor([1.3658, 8.5000, 2.0600], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 13/20\n",
      "Batch: 0 / Epoch: 12 / Loss: 0.0289 / Pred:tensor([1.6939, 8.7695, 3.4194], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 12 / Loss: 0.0424 / Pred:tensor([1.9734, 0.7060, 9.9639], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 12 / Loss: 0.1474 / Pred:tensor([4.3947, 7.0470, 3.2227], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 14/20\n",
      "Batch: 0 / Epoch: 13 / Loss: 0.0384 / Pred:tensor([ 5.2837, 11.4663,  6.7817], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 13 / Loss: 0.0855 / Pred:tensor([ 1.9560,  0.7260, 11.7181], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 13 / Loss: 0.0511 / Pred:tensor([10.6062,  0.0000,  2.0670], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 15/20\n",
      "Batch: 0 / Epoch: 14 / Loss: 0.0901 / Pred:tensor([2.3460, 1.7645, 8.6664], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 14 / Loss: 0.0465 / Pred:tensor([ 4.1494,  2.5859, 11.6663], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 14 / Loss: 0.0060 / Pred:tensor([ 1.6952,  2.3718, 13.7792], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 16/20\n",
      "Batch: 0 / Epoch: 15 / Loss: 0.0717 / Pred:tensor([14.4571,  0.0000,  5.0287], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 15 / Loss: 0.0158 / Pred:tensor([ 4.1841, 10.0949,  2.2427], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 15 / Loss: 0.0128 / Pred:tensor([13.1258,  0.0000,  2.3265], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 17/20\n",
      "Batch: 0 / Epoch: 16 / Loss: 0.0191 / Pred:tensor([ 1.7797, 15.3859,  1.4245], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 16 / Loss: 0.0038 / Pred:tensor([11.0695,  2.8351,  2.8412], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 16 / Loss: 0.0824 / Pred:tensor([ 7.8197, 15.4210,  3.7430], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 18/20\n",
      "Batch: 0 / Epoch: 17 / Loss: 0.0182 / Pred:tensor([ 3.6540,  1.7780, 10.3204], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 17 / Loss: 0.0823 / Pred:tensor([5.0630, 3.2279, 2.1447], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 17 / Loss: 0.0290 / Pred:tensor([11.6287,  0.0000,  3.7479], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 19/20\n",
      "Batch: 0 / Epoch: 18 / Loss: 0.0208 / Pred:tensor([ 1.4412,  0.5759, 12.3510], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 18 / Loss: 0.0246 / Pred:tensor([10.0004,  0.0000,  3.3060], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 18 / Loss: 0.0017 / Pred:tensor([ 4.2366, 13.0526,  1.7050], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Epoch 20/20\n",
      "Batch: 0 / Epoch: 19 / Loss: 0.0833 / Pred:tensor([ 5.1616,  0.0000, 16.1462], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 100 / Epoch: 19 / Loss: 0.0276 / Pred:tensor([ 2.7045,  0.2804, 17.3932], device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "Batch: 200 / Epoch: 19 / Loss: 0.0537 / Pred:tensor([9.9674, 3.1365, 3.4637], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "squeezenet_model = squeezenet1_1(weights='DEFAULT')\n",
    "# redefine networks final classifier\n",
    "squeezenet_model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "squeezenet_model.num_classes = num_classes\n",
    "# send to gpu\n",
    "squeezenet_model = squeezenet_model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=squeezenet_model.parameters(), lr = 1e-5)\n",
    "\n",
    "training_loss = []\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, (b_x, b_y) in enumerate(train_dataloader):\n",
    "        # load data to gpu\n",
    "        b_x = b_x.to(device)\n",
    "        b_y = b_y.to(device)\n",
    "        \n",
    "        y_pred = squeezenet_model(b_x)\n",
    "        loss = loss_fn(y_pred, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # progress_bar.update(1)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            training_loss.append(loss.item())\n",
    "            print(f\"Batch: {i} / Epoch: {epoch} / Loss: {loss.item():.4f} / Pred:{y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/600 [00:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 / Epoch: 0 / Loss: 1.1189 / Pred:tensor([ 0.3714, -0.3363,  0.5475], device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, b_y)\n\u001b[0;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# progress_bar.update(1)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet_model = resnet50(weights='DEFAULT')\n",
    "# get number of in_features from source\n",
    "num_features = resnet_model.fc.in_features\n",
    "# redefine the networks final fully connected layer\n",
    "resnet_model.fc = nn.Linear(num_features, num_classes)\n",
    "# send to gpu\n",
    "resnet_model = resnet_model.to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=resnet_model.parameters(), lr = 1e-5)\n",
    "\n",
    "training_loss = []\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for i, (b_x, b_y) in enumerate(train_dataloader):\n",
    "        # load data to gpu\n",
    "        b_x = b_x.to(device)\n",
    "        b_y = b_y.to(device)\n",
    "        \n",
    "        y_pred = resnet_model(b_x)\n",
    "        loss = loss_fn(y_pred, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # progress_bar.update(1)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            training_loss.append(loss.item())\n",
    "            print(f\"Batch: {i} / Epoch: {epoch} / Loss: {loss.item():.4f} / Pred:{y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, (images, labels) in enumerate(data):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        x = model(images)\n",
    "        value, pred = torch.max(x, 1)\n",
    "        \n",
    "        total += x.size(0)\n",
    "        correct += torch.sum(pred == labels)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Pred: {x} / True: {labels}\")\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: tensor([[11.5583,  1.3501,  3.8475],\n",
      "        [10.9674,  0.0000,  7.3296],\n",
      "        [ 2.7736,  0.0000, 10.9558],\n",
      "        [ 2.4385,  2.9353,  9.2993],\n",
      "        [11.4846,  0.2722,  3.9848],\n",
      "        [ 3.0937,  0.7231, 20.3247],\n",
      "        [ 4.3358,  5.3869, 10.7116],\n",
      "        [ 2.0977,  1.1553, 11.8016],\n",
      "        [ 1.3471,  1.3132, 16.0823],\n",
      "        [ 4.4009,  3.7499, 21.0649],\n",
      "        [ 4.5328, 12.7887,  0.4928],\n",
      "        [ 8.3177,  2.5135,  4.8718],\n",
      "        [ 1.6772,  0.8711, 14.2273],\n",
      "        [ 4.8196, 14.5257,  4.3074],\n",
      "        [ 3.3409,  1.1962, 19.3875],\n",
      "        [ 7.5941, 18.9709,  2.1439],\n",
      "        [ 2.6958, 12.3721,  0.9817],\n",
      "        [ 3.0802,  0.7648, 15.0735],\n",
      "        [ 2.5445,  2.8574, 12.0561],\n",
      "        [18.6187,  0.0000,  3.3410],\n",
      "        [17.0872,  0.0000,  4.3105],\n",
      "        [ 5.9323, 16.5959,  1.1664],\n",
      "        [13.3630,  0.1723,  4.1763],\n",
      "        [ 4.1997, 11.1540,  2.9674],\n",
      "        [18.3207,  0.0000,  4.9398],\n",
      "        [17.9952,  0.6253,  1.0803],\n",
      "        [ 1.7773,  2.5519, 18.4166],\n",
      "        [ 0.7917,  0.2361, 18.9734],\n",
      "        [16.1238,  0.4380,  6.3243],\n",
      "        [ 1.4897,  9.5036,  1.5332],\n",
      "        [ 6.8646, 15.0062,  3.6433],\n",
      "        [ 2.2286,  8.2007,  3.9306]], device='cuda:0', grad_fn=<ViewBackward0>) / True: tensor([0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 1, 0, 2, 1, 2, 1, 1, 2, 2, 0, 0, 1, 0, 1,\n",
      "        0, 0, 2, 2, 0, 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9775, device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(squeezenet_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: tensor([[ 5.6440, -3.0842, -3.4319],\n",
      "        [-3.6565, -1.1003,  3.9657],\n",
      "        [-2.8765, -3.7890,  6.3882],\n",
      "        [-1.3958, -2.7837,  3.0938],\n",
      "        [ 5.2728, -3.2567, -2.8001],\n",
      "        [-4.7179, -2.2842,  5.0167],\n",
      "        [ 8.2861, -4.6745, -4.9196],\n",
      "        [-2.9848, -1.5646,  3.3226],\n",
      "        [-4.0698, -3.0370,  5.0627],\n",
      "        [10.0510, -5.4838, -6.5463],\n",
      "        [ 7.5975, -4.2481, -5.3943],\n",
      "        [-2.9406, -1.5379,  3.1714],\n",
      "        [-3.8076, -4.7783,  5.5296],\n",
      "        [-5.1384,  8.2579, -6.0299],\n",
      "        [-2.7337,  4.6717, -3.5344],\n",
      "        [ 5.1528, -2.5423, -4.4035],\n",
      "        [-2.3690, -1.9375,  2.6353],\n",
      "        [-2.7639, -2.8844,  4.2537],\n",
      "        [-2.3243, -2.2724,  3.6969],\n",
      "        [-2.7084, -5.9433,  7.5668],\n",
      "        [-4.9801,  8.8308, -5.6634],\n",
      "        [-2.4452, -2.1073,  3.5269],\n",
      "        [-5.2221,  8.5850, -4.9798],\n",
      "        [-5.0051, -2.4136,  5.9758],\n",
      "        [-4.5060,  8.0058, -4.8908],\n",
      "        [ 6.3094, -3.7759, -3.9550],\n",
      "        [-4.9534,  8.8179, -5.3487],\n",
      "        [ 5.2502, -3.6872, -3.5377],\n",
      "        [ 6.4189, -4.5086, -3.2411],\n",
      "        [-5.6195,  9.9916, -6.0465],\n",
      "        [-3.5197, -3.1645,  4.7582],\n",
      "        [ 9.9778, -6.0505, -5.8296]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) / True: tensor([0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 2, 1, 1, 0, 2, 2, 2, 2, 1, 2, 1, 2,\n",
      "        1, 0, 1, 0, 0, 1, 2, 0], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9888, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(resnet_model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
